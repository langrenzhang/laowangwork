
创建项目   scrapy startproject tutorial


这些文件分别是:
scrapy.cfg: 项目的配置文件
tutorial/: 该项目的python模块。之后您将在此加入代码。
tutorial/items.py: 项目中的item文件.
tutorial/pipelines.py: 项目中的pipelines文件.
tutorial/settings.py: 项目的设置文件.
tutorial/spiders/: 放置spider代码的目录.


爬取 进入项目的根目录，执行下列命令启动spider:  scrapy crawl dmoz


提取Item Selectors选择器简介:
从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors 。 关于selector和其他提取机制的信息请参考 Selector文档 。
这里给出XPath表达式的例子及对应的含义:
/html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素
/html/head/title/text(): 选择上面提到的 <title> 元素的文字
//td: 选择所有的 <td> 元素
//div[@class="mine"]: 选择所有具有 class="mine" 属性的 div 元素


在Shell中尝试Selector选择器:
scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"
当shell载入后，您将得到一个包含response数据的本地 response 变量。输入 response.body 将输出response的包体， 输出 response.headers 可以看到response的包头。
更为重要的是，当输入 response.selector 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 response.selector.xpath() 、 response.selector.css() 的 快捷方法(shortcut): response.xpath() 和 response.css() 。
同时，shell根据response提前初始化了变量 sel 。
可以试试：
sel.xpath('//title') 
sel.xpath('//title').extract()
sel.xpath('//title/text()')
sel.xpath('//title/text()').re('(\w+):')


保存爬取到的数据 最简单存储爬取的数据的方式是使用 Feed exports:
scrapy crawl dmoz -o items.json


命令行工具(Command line tools)

创建项目
一般来说，使用 scrapy 工具的第一件事就是创建您的Scrapy项目:
scrapy startproject myproject

控制项目
您可以在您的项目中使用 scrapy 工具来对其进行控制和管理。
比如，创建一个新的spider:
scrapy genspider mydomain mydomain.com

您也可以查看所有可用的命令:
scrapy -h

全局命令:

startproject
settings
runspider
shell
fetch
view
version

项目(Project-only)命令:
crawl
check
list
edit
parse
genspider
deploy
bench

########################################################################################
命令详解：
startproject
语法: scrapy startproject <project_name>
是否需要项目: no
在 project_name 文件夹下创建一个名为 project_name 的Scrapy项目。

例子:

$ scrapy startproject myproject
genspider
语法: scrapy genspider [-t template] <name> <domain>
是否需要项目: yes
在当前项目中创建spider。

这仅仅是创建spider的一种快捷方法。该方法可以使用提前定义好的模板来生成spider。您也可以自己创建spider的源码文件。

例子:

$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider -d basic
import scrapy

class $classname(scrapy.Spider):
    name = "$name"
    allowed_domains = ["$domain"]
    start_urls = (
        'http://www.$domain/',
        )

    def parse(self, response):
        pass

$ scrapy genspider -t basic example example.com
Created spider 'example' using template 'basic' in module:
  mybot.spiders.example
crawl
语法: scrapy crawl <spider>
是否需要项目: yes
使用spider进行爬取。

例子:

$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
check
语法: scrapy check [-l] <spider>
是否需要项目: yes
运行contract检查。

例子:

$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
list
语法: scrapy list
是否需要项目: yes
列出当前项目中所有可用的spider。每行输出一个spider。

使用例子:

$ scrapy list
spider1
spider2
edit
语法: scrapy edit <spider>
是否需要项目: yes
使用 EDITOR 中设定的编辑器编辑给定的spider

该命令仅仅是提供一个快捷方式。开发者可以自由选择其他工具或者IDE来编写调试spider。

例子:

$ scrapy edit spider1
fetch
语法: scrapy fetch <url>
是否需要项目: no
使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。

该命令以spider下载页面的方式获取页面。例如，如果spider有 USER_AGENT 属性修改了 User Agent，该命令将会使用该属性。

因此，您可以使用该命令来查看spider如何获取某个特定页面。

该命令如果非项目中运行则会使用默认Scrapy downloader设定。

例子:

$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
view
语法: scrapy view <url>
是否需要项目: no
在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。

例子:

$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
shell
语法: scrapy shell [url]
是否需要项目: no
以给定的URL(如果给出)或者空(没有给出URL)启动Scrapy shell。 查看 Scrapy终端(Scrapy shell) 获取更多信息。

例子:

$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]
parse
语法: scrapy parse <url> [options]
是否需要项目: yes
获取给定的URL并使用相应的spider分析处理。如果您提供 --callback 选项，则使用spider的该方法处理，否则使用 parse 。

支持的选项:

--spider=SPIDER: 跳过自动检测spider并强制使用特定的spider
--a NAME=VALUE: 设置spider的参数(可能被重复)
--callback or -c: spider中用于解析返回(response)的回调函数
--pipelines: 在pipeline中处理item
--rules or -r: 使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数
--noitems: 不显示爬取到的item
--nolinks: 不显示提取到的链接
--nocolour: 避免使用pygments对输出着色
--depth or -d: 指定跟进链接请求的层次数(默认: 1)
--verbose or -v: 显示每个请求的详细信息
例子:

$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': u'Example item',
 'category': u'Furniture',
 'length': u'12 cm'}]

# Requests  -----------------------------------------------------------------
[]
settings
语法: scrapy settings [options]
是否需要项目: no
获取Scrapy的设定

在项目中运行时，该命令将会输出项目的设定值，否则输出Scrapy默认设定。

例子:

$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
runspider
语法: scrapy runspider <spider_file.py>
是否需要项目: no
在未创建项目的情况下，运行一个编写在Python文件中的spider。

例子:

$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
version
语法: scrapy version [-v]
是否需要项目: no
输出Scrapy版本。配合 -v 运行时，该命令同时输出Python, Twisted以及平台的信息，方便bug提交。

deploy
0.11 新版功能.

语法: scrapy deploy [ <target:project> | -l <target> | -L ]
是否需要项目: yes
将项目部署到Scrapyd服务。查看 部署您的项目 。

bench
0.17 新版功能.

语法: scrapy bench
是否需要项目: no
运行benchmark测试。 Benchmarking 。

自定义项目命令
您也可以通过 COMMANDS_MODULE 来添加您自己的项目命令。您可以以 scrapy/commands 中Scrapy commands为例来了解如何实现您的命令。

COMMANDS_MODULE

Default: '' (empty string)

用于查找添加自定义Scrapy命令的模块。

例子:

COMMANDS_MODULE = 'mybot.commands'


###############################################
